{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>NLP Project</center></h1>\n",
    "<h3><center>Spring 2022</center></h3>\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Please don't forget to fill in this data </u>\n",
    "**Member 1**\n",
    "\n",
    "Name:Youssef El Manyalawy\n",
    "\n",
    "GUC-ID:43-0425\n",
    "\n",
    "**Member 2**\n",
    "\n",
    "Name: Malak Mohamed\n",
    "\n",
    "GUC-ID: 43-0272\n",
    "\n",
    "**Member 3**\n",
    "\n",
    "Name: Yasser Bahaa\n",
    "\n",
    "GUC-ID: 43-3378\n",
    "\n",
    "**Member 4 (Optional)**\n",
    "\n",
    "Name: Mohamed Abdelhamid\n",
    "\n",
    "GUC-ID: 43-1637\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "## Preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(file_name):\n",
    "#     Open file an conver it to string\n",
    "    data = \"\"\n",
    "    with open(\"./scripts/\" + file_name, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "#     Remove extra spaces and numbers, and change string to lower case, and remove all single characters\n",
    "    data = re.sub(' +', ' ', data)\n",
    "    data = re.sub(r'\\d+', '', data)\n",
    "    data = re.sub(r'\\b\\w\\b', ' ', data)\n",
    "    data = data.lower()\n",
    "#     tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_tokens = tokenizer.tokenize(data)\n",
    "    data = [word for word in text_tokens if word not in stopwords.words('english')]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "## VAD Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_dict(file_name):\n",
    "    d = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            word = line.split()\n",
    "            if(len(word)==2):\n",
    "                d[word[0]] = word[1]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_vad(vec):\n",
    "    a = []\n",
    "    v = []\n",
    "    d = []\n",
    "    v_file = file_to_dict(\"v-scores.txt\")\n",
    "    a_file = file_to_dict(\"a-scores.txt\")\n",
    "    d_file = file_to_dict(\"d-scores.txt\")\n",
    "    for word in vec:\n",
    "        if word in v_file:\n",
    "            v.append(float(v_file[word]))\n",
    "        if word in a_file:\n",
    "            a.append(float(a_file[word]))\n",
    "        if word in d_file:\n",
    "            d.append(float(d_file[word]))\n",
    "        \n",
    "    return v,a,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average500(vec):\n",
    "    i = 0\n",
    "    res = []\n",
    "    while True:\n",
    "        temp = statistics.mean(vec[i : i+501])\n",
    "        res.append(temp)\n",
    "        i = i + 500\n",
    "        if(i>len(vec)):\n",
    "            break\n",
    "    return res  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n",
    "\n",
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(filename):\n",
    "    script = data_cleaning(filename)\n",
    "    v,a,d = vector_to_vad(script)\n",
    "    v = average500(v)\n",
    "    a = average500(a)\n",
    "    d = average500(d)\n",
    "    num_rows = len(v)\n",
    "    time = list(range(0,num_rows))\n",
    "    data = pd.DataFrame({\n",
    "    'Time': time, \n",
    "    'V': v,\n",
    "    'A': a,\n",
    "    'D': d})\n",
    "    fig = sns.lineplot(x='Time', y='value', hue='variable', \n",
    "             data=pd.melt(data, ['Time']))\n",
    "    fig.figure.savefig('graphs/' + filename.split(\".\")[0] + \".jpg\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"./scripts\"):\n",
    "    pipeline(filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
